{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load libs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data\n",
    "train=pd.read_csv('../input/train.csv')\n",
    "test=pd.read_csv('../input/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "df_test=test.drop(['ID_code'], axis=1)\n",
    "df_test = df_test.values\n",
    "unique_samples = []\n",
    "unique_count = np.zeros_like(df_test)\n",
    "for feature in tqdm(range(df_test.shape[1])):\n",
    "    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n",
    "    unique_count[index_[count_ == 1], feature] += 1\n",
    "\n",
    "# Samples which have unique values are real the others are fake\n",
    "real_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n",
    "synthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n",
    "\n",
    "df_test_real = df_test[real_samples_indexes].copy()\n",
    "df_test_real=pd.DataFrame(df_test_real)\n",
    "df_test_real=df_test_real.add_prefix('var_')\n",
    "df_test_real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "train_value=train.drop(['ID_code', 'target'], axis=1)\n",
    "df_combined=pd.concat([train_value, df_test_real])\n",
    "del train_value\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sortedcontainers import SortedDict\n",
    "# dictionary=df_combined['var_0'].value_counts().to_dict()\n",
    "# np.diff(np.array(sorted(dictionary)))\n",
    "#np.array(sorted(dictionary.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: \n",
    "\n",
    "- `We take counts from combined train+real test data and map to the original test and train. `\n",
    "\n",
    "- `We assign a mathematical transformations as well. I used log, but others such as sqrt, etc can also work.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    var='var_'+str(i)\n",
    "    if i%25==0:\n",
    "        print (i)\n",
    "    dictionary=df_combined[var].value_counts().to_dict()\n",
    "    dictionary2=df_combined[var].sort_values().diff(-1).to_dict()\n",
    "\n",
    "    train['Ncount_'+var]   = train[var].map(dictionary)\n",
    "    train['cMap_'+var]     = train[var]*np.log2(train['Ncount_'+var]+1)\n",
    "    train['cMap2_'+var]    = train[var]/np.log2(train['Ncount_'+var]+1)\n",
    "    train['cDiff_'+var]    = train.index.map(dictionary2)\n",
    "    train.drop('Ncount_'+var, inplace=True, axis=1)\n",
    "    \n",
    "    dictionary3=df_test_real[var].sort_values().diff(-1).to_dict()\n",
    "    test['Ncount_'+var] =  test[var].map(dictionary)\n",
    "    test['cMap_'+var]   =  test[var]*np.log2(test['Ncount_'+var]+1)\n",
    "    test['cMap2_'+var]  =  test[var]/np.log2(test['Ncount_'+var]+1)\n",
    "    test['cDiff_'+var]  =  test.index.map(dictionary3)\n",
    "    test.drop('Ncount_'+var, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_combined\n",
    "gc.collect()\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import gc\n",
    "\n",
    "for i in tqdm(range(200)):\n",
    "    var='var_'+str(i)\n",
    "    if i%25==0:\n",
    "        print (i)\n",
    "\n",
    "    mask = np.zeros(train.shape[0])\n",
    "    Bool = (train['cMap_'+var] == train['cMap2_'+var])\n",
    "    Bool1 = (train['cMap_'+var] > train['cMap2_'+var])\n",
    "    Bool2 = (train['cMap_'+var] < train['cMap2_'+var])\n",
    "                 \n",
    "    mask[Bool] = 1   ### equal\n",
    "    mask[Bool1] = 2  ### Greater than\n",
    "    mask[Bool2] = 3  ### less than\n",
    "    train['Test_eq_test1_'+var] = mask\n",
    "    train['Abs_Diff_test_test1_'+var] = abs(train['cMap_'+var] - train['cMap2_'+var])\n",
    "    \n",
    "    uq1 = np.unique(train['cMap_'+var])\n",
    "    uq2 = np.unique(train['cMap2_'+var])\n",
    "    \n",
    "    train[var+'_perc1'] = [stats.percentileofscore(uq1, a, 'weak') for a in train[var].values]    \n",
    "    train[var+'_perc2'] = [stats.percentileofscore(uq2, a, 'weak') for a in train[var].values]        \n",
    "    \n",
    "    mask = np.zeros(test.shape[0])\n",
    "    Bool = (test['cMap_'+var] == test['cMap2_'+var])\n",
    "    Bool1 = (test['cMap_'+var] > test['cMap2_'+var])\n",
    "    Bool2 = (test['cMap_'+var] < test['cMap2_'+var])\n",
    "    \n",
    "    mask[Bool] = 1   ### equal\n",
    "    mask[Bool1] = 2  ### Greater than\n",
    "    mask[Bool2] = 3  ### less than\n",
    "    test['Test_eq_test1_'+var] = mask\n",
    "    test['Abs_Diff_test_test1_'+var] = abs(test['cMap_'+var] - test['cMap2_'+var])\n",
    "\n",
    "    uq1 = np.unique(test['cMap_'+var])\n",
    "    uq2 = np.unique(test['cMap2_'+var])\n",
    "\n",
    "    test[var+'_perc1'] = [stats.percentileofscore(uq1, a, 'weak') for a in test[var].values]    \n",
    "    test[var+'_perc2'] = [stats.percentileofscore(uq2, a, 'weak') for a in test[var].values]    \n",
    "    del mask, Bool1,Bool,Bool2,uq1,uq2\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',100)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_code=test['ID_code']\n",
    "X_test = test.drop(drop,axis = 1)\n",
    "X_test = X_test.drop(['ID_code'],axis = 1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=train['target']\n",
    "X = train.drop(drop, axis=1)\n",
    "X = X.drop(['target', 'ID_code'], axis=1)\n",
    "del train\n",
    "X.head()\n",
    "X.shape\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in X.columns if c not in ['ID_code', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_param = {\n",
    "    'bagging_freq': 5,\n",
    "        'bagging_fraction': 0.35,\n",
    "        'boost_from_average':'false',\n",
    "        'boost': 'gbdt',\n",
    "        'feature_fraction': 0.0193,\n",
    "        'learning_rate': 0.0291,\n",
    "        'max_depth': -1,\n",
    "        'metric':'auc',\n",
    "        'min_data_in_leaf': 80,\n",
    "        'min_sum_hessian_in_leaf': 12,\n",
    "        'num_leaves': 8,    ### try 2; no interaction between variables makes sense here due to IID\n",
    "        'tree_learner': 'serial',\n",
    "        'objective': 'binary', \n",
    "        'verbosity': 1,\n",
    "        \"boost_from_average\": \"false\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 7 # Number of K-fold Splits\n",
    "splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True ,random_state=1111).split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = np.zeros(len(X))\n",
    "predictions = np.zeros(len(X_test))\n",
    "feature_importance_df=pd.DataFrame()\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):  \n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "\n",
    "    x=np.array(X)\n",
    "    y=np.array(y)\n",
    "    \n",
    "    trn_data = lgb.Dataset(x[train_idx], label=y[train_idx])\n",
    "    val_data = lgb.Dataset(x[valid_idx], label=y[valid_idx])\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n",
    "                 \n",
    "    lgb_clf=lgb.train(lgb_param, trn_data, 100000, valid_sets = [trn_data, val_data], early_stopping_rounds=10000, verbose_eval=1000)\n",
    "    \n",
    "    oof[valid_idx] = lgb_clf.predict(x[valid_idx], num_iteration=lgb_clf.best_iteration)\n",
    "        \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = i+1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += lgb_clf.predict(X_test, num_iteration=lgb_clf.best_iteration) /n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(y, oof)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 600)\n",
    "fold_importance_df.groupby('feature')['importance'].sort_values(by='importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.DataFrame()\n",
    "submission['ID_code']=ID_code\n",
    "submission['target']=predictions\n",
    "submission.to_csv('submission_lgb_freq_log2_eqfeats_v1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('submission_lgb_freq_log2_eqfeats_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### End of analysis ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
